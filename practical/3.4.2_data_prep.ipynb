{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4.2 Data Preparation for Integrating Sources\n",
    "\n",
    "This section teaches how to integrate multiple data sources to obtain more dimensions for analysis and modeling. To practically learn how to implement the methods, we return to the mini-case study of customer churn. We have observed three available datasets. The following summarizes the datasets, their source file name, and the unique key (i.e., the attribute that its value can uniquely identify a row of a dataset)\n",
    "\n",
    "- Customer (customer.xlsx): the unique key is the CustomerId attribute.\n",
    "- Churn (churn.xlsx): the unique key is the CustomerId attribute.\n",
    "- Payment transaction (ptransaction.xlsx): the unique key is the CustomerID attribute.\n",
    "\n",
    "Based on the unique key of each dataset of Customer, Churn, and Payment transaction, we can join them together using the CustomerId attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Identifying and Removing Duplication\n",
    "Before joining the datasets, it is essential to check if duplications exist to avoid redundancy. The following example of Python codes can be used to find duplications with the duplicated() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [CustomerId, Firstname, Gender, PostalCode, HashCode, Birthdate]\n",
      "Index: []\n",
      "   CustomerId\n",
      "4           6\n",
      "Empty DataFrame\n",
      "Columns: [CustomerId, LastTrxDate, MinTrxValue, MaxTrxValue, TotalTrxValue, Cash, Cheque, CreditCard, SinceLastTrx]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# import library and access data sources\n",
    "import pandas as pd\n",
    "import math\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "df1 = pd.read_excel ('data/customer.xlsx')\n",
    "df2 = pd.read_excel ('data/churn.xlsx')\n",
    "df3 = pd.read_excel ('data/ptransaction.xlsx')\n",
    "\n",
    "# identify if duplication exists \n",
    "print(df1[df1.duplicated()])\n",
    "print(df2[df2.duplicated()])\n",
    "print(df3[df3.duplicated()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are duplications, then we can use the following example codes to remove the duplications using the drop_duplicates() function to find duplicates and the len() to count the number of duplicates that exist. If none, then proceed to the data types exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if duplication exists, remove and keep only one \n",
    "if len(df1[df1.duplicated()]) > 0: \n",
    "    df1 = df1.drop_duplicates(keep = 'first') \n",
    "if len(df2[df2.duplicated()]) > 0: \n",
    "    df2 = df2.drop_duplicates(keep = 'first') \n",
    "if len(df3[df3.duplicated()]) > 0: \n",
    "    df3 = df3.drop_duplicates(keep = 'first') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying and Converting Unmatched Data Types \n",
    "As we have observed the data type of each attribute in the Customer dataset in the previous section 4.1, four attributes' data types need to be corrected. The Firstname, HashCode, PostalCode and Gender should be set as string type. The following Python commands serve the conversion for correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data type \n",
    "df1['Firstname'] = df1['Firstname'].astype('string') \n",
    "df1['PostalCode'] = df1['PostalCode'].astype('string') \n",
    "df1['HashCode'] = df1['HashCode'].astype('string') \n",
    "df1['Gender'] = df1['Gender'].astype('string')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a New Attribute to Indicate the Target 'Churn'\n",
    "The Churn dataset contains all the `CustomerId` of customers that have already churned but do not have an attribute to indicate the classification target, i.e. Churn or not Churn. For this case study, we use the value of 1 to indicate `churn`, else 0.  \n",
    "\n",
    "To have an indicator of customer churn, we create a new attribute called Churn (i.e. Dummy attribute) as the indicator with a value equal to 'yes' denotes a churned customer. Next, declare the data type of the new attribute to the character type using the astype('string') function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dummy variable, naming it Churn attribute to indicate churn with a value 'yes'\n",
    "df2['Churn'] = 'yes' \n",
    "df2['Churn'] = df2['Churn'].astype('string')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a New Derived Attribute for Age \n",
    "In the `Customer` dataset, we observe the `Birthdate` attribute. However, the attribute carries a hidden meaning of age. Therefore, creating and deriving a new attribute Age, to represent age better reflects the meaning. Because the case study data was collected until the end of February 2022, we calculate a customer's age based on this timeline. The following codes show the formula of age calculation according to the timeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Age Attribute deriving from Birthdate.\n",
    "df1['Age'] = (datetime(2022, 3, 1) - df1['Birthdate']) // timedelta(days=365.2425)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrating Data Sources\n",
    "We can use the Python `merge()` function to integrate all data sources. We perform a left join to merge datasets for the implementation by ensuring all rows from the Customer dataset are captured. We want to perform churn classification modeling based on customers' profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns all rows from the Customer, even if there are no matches in Churn dataset\n",
    "df_merge = df1.merge(df2, on='CustomerId', how='left')\n",
    "\n",
    "#returns all rows from the Customer, even if there are no matches in Transaction\n",
    "df_merge = df_merge.merge(df3, on='CustomerId', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Pre-processing to ensure Data Quality\n",
    "To further prepare the merged dataset, there several pre-processing tasks are necessary. Observe the missing values of each attribute after the merge using the `info()` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 16 columns):\n",
      " #   Column         Non-Null Count  Dtype         \n",
      "---  ------         --------------  -----         \n",
      " 0   CustomerId     1000 non-null   int64         \n",
      " 1   Firstname      1000 non-null   string        \n",
      " 2   Gender         999 non-null    string        \n",
      " 3   PostalCode     1000 non-null   string        \n",
      " 4   HashCode       1000 non-null   string        \n",
      " 5   Birthdate      999 non-null    datetime64[ns]\n",
      " 6   Age            999 non-null    float64       \n",
      " 7   Churn          503 non-null    string        \n",
      " 8   LastTrxDate    998 non-null    datetime64[ns]\n",
      " 9   MinTrxValue    998 non-null    float64       \n",
      " 10  MaxTrxValue    998 non-null    float64       \n",
      " 11  TotalTrxValue  998 non-null    float64       \n",
      " 12  Cash           998 non-null    float64       \n",
      " 13  Cheque         998 non-null    float64       \n",
      " 14  CreditCard     998 non-null    float64       \n",
      " 15  SinceLastTrx   998 non-null    float64       \n",
      "dtypes: datetime64[ns](2), float64(8), int64(1), string(5)\n",
      "memory usage: 125.1 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# observe the dataset attributes and data types\n",
    "print(df_merge.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the `info()` result, several attributes have missing values. Next, we examine how these missing values can be treated according to the meaning of each attribute:\n",
    "\n",
    "- When we created a dummy variable, i.e., the Churn attribute in the Churn dataset, we only captured those customers who have already churned. During merging between Customer and Churn datasets, only customers who have churned contain a value of 1. Those not churned have a null value. We will replace the null or missing value with 0, indicating the non-churn status.\n",
    "- The payment transaction dataset does not contain all customers' payment details, i.e., some customers do not have payment transactions. Also, we observe that one Customer dataset row does not contain gender information. Consequently, null or missing values will exist in the merged dataset. To tackle this problem, we can either delete those rows having missing values or replace the null data with a value. Based on the problem understanding, the classification requires understanding customers' demographics and payment transactions' impact on churn. Thus, we will delete the rows that have missing transaction data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying and Treating Missing Values\n",
    "Besides the `info()` function, we can use another option to explore the existence of the missing values in a specific attribute using the `isna()` function. The example codes are as follows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomerId 0\n",
      "Firstname 0\n",
      "Gender 1\n",
      "PostalCode 0\n",
      "HashCode 0\n",
      "Birthdate 1\n",
      "Age 1\n",
      "Churn 497\n",
      "LastTrxDate 2\n",
      "MinTrxValue 2\n",
      "MaxTrxValue 2\n",
      "TotalTrxValue 2\n",
      "Cash 2\n",
      "Cheque 2\n",
      "CreditCard 2\n",
      "SinceLastTrx 2\n"
     ]
    }
   ],
   "source": [
    "for i, col in enumerate(df_merge.columns): \n",
    "    print(col, df_merge[col].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the `info()` and `isna()` functions, `info()` gives the number of not null values and information on the data type of each attribute in a dataset, whereas `isna()` simply shows the number of missing values that exist in each attribute.\n",
    "\n",
    "We perform the following tasks based on the rationale for pre-processing using the fillna() function to identify missing values/null and fill in a value into a specific attribute's data. \n",
    "\n",
    "We use the `apply(lambda x : math.floor(x)` function to set the data type of the attributes to an integer, and use a 'no' value for the Churn attribute to replace the missing values for those customers who have not churned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to treat null or missing values \n",
    "df_merge['Churn'] = df_merge['Churn'].fillna('no') \n",
    "df_merge['Cash'] = df_merge['Cash'].fillna(0)\n",
    "df_merge['Cash'] = df_merge['Cash'].apply(lambda x : math.floor(x))\n",
    "df_merge['CreditCard'] = df_merge['CreditCard'].fillna(0)\n",
    "df_merge['CreditCard'] = df_merge['CreditCard'].apply(lambda x : math.floor(x))\n",
    "df_merge['Cheque'] = df_merge['Cheque'].fillna(0)\n",
    "df_merge['Cheque'] = df_merge['Cheque'].apply(lambda x : math.floor(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now treated missing values with a value of 0 to reflect their meaning in the respective attributes. For the rest of the attributes containing missing values, such as `Gender`, `LastTrxDate`, and `SinceLastTrx` (derived from the `LastTrxDate`), we delete the rows because replacing them with any value in the data is meaningless.\n",
    "\n",
    "We use the `dropna()` function to drop all the rows with at least one missing in any attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to delete the remaining rows with missing values in any attribute\n",
    "df_merge.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing Data Transformation using Replacement\n",
    "In a real-world scenario, a postal code represents a specific area. For example, we may use the `PostalCode` to identify a customer's living area. However, directly taking the original postal code with many levels (i.e., different values) in the `Customer` dataset may cause the classification model's ability to generalize the future data for prediction. This problem is called overfitting. \n",
    "\n",
    "`We will look at the overfitting problem in mode detail in Week 6. `\n",
    "\n",
    "To avoid overfitting problems due to too many levels in the PostalCode attribute, we use the first character of the postal code to analyze a broader area instead of discarding the information totally. To realize this deployment, we replace the original postal code with the first character of the value.\n",
    "\n",
    "To select the first character of the attribute, we use the following Python code to implement the selection and set it as a string type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to select the first character of the PostalCode\n",
    "df_merge['PostalCode'] = df_merge['PostalCode'].str[0].astype('string')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Relevant Attributes\n",
    "\n",
    "The previous steps have integrated all data sources and prepared the attributes to reflect the correct meaning and data types. However, some irrelevant such as `HashCode`, represent a meaningless computer-generated value. Moreover, a duplicated attribute such as `Birthdate` that the `Age` has better reflects the meaning. Besides, the `LastTrxDate` values have been derived and reflected in the `SinceLastTrx` attribute. Thus, we will exclude these three attributes and select the rest of the other attributes.\n",
    "\n",
    "We use the following codes to implement the selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to select useful attributes for further analysis and modeling\n",
    "df_merge = df_merge[['CustomerId','Gender', 'Age','PostalCode', 'MinTrxValue', \n",
    "                'MaxTrxValue', 'TotalTrxValue', 'Cash', 'CreditCard', 'Cheque', \n",
    "                'SinceLastTrx', 'Churn']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Consolidated Data\n",
    "We have now completed the data preparation tasks for the case study to bring the consolidated data for further analysis. Observe the final dataset dimension, attributes, and data types to confirm their structure for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(996, 12)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 996 entries, 0 to 999\n",
      "Data columns (total 12 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   CustomerId     996 non-null    int64  \n",
      " 1   Gender         996 non-null    string \n",
      " 2   Age            996 non-null    float64\n",
      " 3   PostalCode     996 non-null    string \n",
      " 4   MinTrxValue    996 non-null    float64\n",
      " 5   MaxTrxValue    996 non-null    float64\n",
      " 6   TotalTrxValue  996 non-null    float64\n",
      " 7   Cash           996 non-null    int64  \n",
      " 8   CreditCard     996 non-null    int64  \n",
      " 9   Cheque         996 non-null    int64  \n",
      " 10  SinceLastTrx   996 non-null    float64\n",
      " 11  Churn          996 non-null    string \n",
      "dtypes: float64(5), int64(4), string(3)\n",
      "memory usage: 101.2 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# observe the consolidated data dimension and data types\n",
    "print(df_merge.shape)\n",
    "print(df_merge.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we save the data into a new and final dataset file using the following example Python code, naming it CustomerChurn.csv in a CVS format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data into a cvs file\n",
    "df_merge.to_csv('data/CustomerChurn.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe the file directory, the `CustomerChurn.csv` should be available for download and view. \n",
    "\n",
    "`We will use the CustomerChurn.csv dataset for future analysis and subsequent phases of the case study.`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
